{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0739d3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 06:45:33.765399: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-07 06:45:33.765462: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8994b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('pima-indians-diabetes.data.csv')\n",
    "#X = dataset[:8]\n",
    "#y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c6d5ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>148</th>\n",
       "      <th>72</th>\n",
       "      <th>35</th>\n",
       "      <th>0</th>\n",
       "      <th>33.6</th>\n",
       "      <th>0.627</th>\n",
       "      <th>50</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   6  148  72  35    0  33.6  0.627  50  1\n",
       "0  1   85  66  29    0  26.6  0.351  31  0\n",
       "1  8  183  64   0    0  23.3  0.672  32  1\n",
       "2  1   89  66  23   94  28.1  0.167  21  0\n",
       "3  0  137  40  35  168  43.1  2.288  33  1\n",
       "4  5  116  74   0    0  25.6  0.201  30  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33834899",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds = RegressionColumnarDataset(X_train, cat_cols, y_train) #type: __main__.RegressionColumnarDataset\n",
    "valds = RegressionColumnarDataset(X_val, cat_cols, y_val) #type: __main__.RegressionColumnarDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionColumnarDataset(data.Dataset):\n",
    "    def __init__(self, df, cats, y):\n",
    "        \n",
    "        \n",
    "        self.dfcats = df[cats] #type: pandas.core.frame.DataFrame\n",
    "        self.dfconts = df.drop(cats, axis=1) #type: pandas.core.frame.DataFrame\n",
    "        \n",
    "        \n",
    "        self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(np.int64) #tpye: numpy.ndarray\n",
    "        self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(np.float32) #tpye: numpy.ndarray\n",
    "        self.y = y.values.astype(np.float32)\n",
    "        \n",
    "        \n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return [self.cats[idx], self.conts[idx], self.y[idx]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b55019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help functions\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "def bn_drop_lin(n_in:int, n_out:int, bn:bool=True, p:float=0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers\n",
    "\n",
    "def ifnone(a,b):\n",
    "    \"`a` if `a` is not None, otherwise `b`.\"\n",
    "    return b if a is None else a\n",
    "\n",
    "def listify(p, q):\n",
    "    \"Make `p` listy and the same length as `q`.\"\n",
    "    if p is None: p=[]\n",
    "    elif isinstance(p, str):          p = [p]\n",
    "    elif not isinstance(p, Iterable): p = [p]\n",
    "    #Rank 0 tensors in PyTorch are Iterable but don't have a length.\n",
    "    else:\n",
    "        try: a = len(p)\n",
    "        except: p = [p]\n",
    "    n = q if type(q)==int else len(p) if q is None else len(q)\n",
    "    if len(p)==1: p = p * n\n",
    "    assert len(p)==n, f'List len mismatch ({len(p)} vs {n})'\n",
    "    return list(p)\n",
    "\n",
    "              \n",
    "\n",
    "class TabularModel(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, emb_szs, n_cont:int, out_sz:int, layers, ps=None,\n",
    "                 emb_drop:float=0., y_range=None, use_bn:bool=True, bn_final:bool=False):\n",
    "        super().__init__()\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs]) #type: torch.nn.modules.container.ModuleList\n",
    "        self.emb_drop = nn.Dropout(emb_drop) #type: torch.nn.modules.dropout.Dropout\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont) #type torch.nn.modules.batchnorm.BatchNorm1d\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds) # n_emb = 17 , type: int\n",
    "        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
    "        sizes = [n_emb + n_cont] + layers + [out_sz] #typeL list, len: 4\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None] #type: list, len: 3.  the last in None because we finish with linear\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers) #type: torch.nn.modules.container.Sequential\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)] #take the embedding list and grab an embedding and pass in our single row of data.        \n",
    "            x = torch.cat(x, 1) # concatenate it on dim 1 ## remeber that the len is the batch size\n",
    "            x = self.emb_drop(x) # pass it through a dropout layer\n",
    "        if self.n_cont != 0:\n",
    "            x_cont = self.bn_cont(x_cont) # batchnorm1d\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont # combine the categircal and continous variables on dim 1\n",
    "        x = self.layers(x)\n",
    "        if self.y_range is not None:\n",
    "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0] # deal with y_range\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77601ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range = (0, y_train.max()*1.2)\n",
    "model = TabularModel(emb_szs = emb_szs,n_cont = len(cont_cols),out_sz = 1,layers = [1000,500,250],ps= [0.001,0.01,0.01],emb_drop=0.04, y_range=y_range).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2) # can add: weight_decay=\n",
    "\n",
    "lr = defaultdict(list)\n",
    "tloss = defaultdict(list)\n",
    "vloss = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c415760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_y(y): return np.exp(y)\n",
    "\n",
    "#def rmse(targ, y_pred):\n",
    "#    return np.sqrt(mean_squared_error(inv_y(y_pred), inv_y(targ))) #.detach().numpy()\n",
    "    \n",
    "\n",
    "#def rmse(targ, y_pred):\n",
    "#   return np.sqrt(mean_squared_error(y_pred, targ)) #.detach().numpy()\n",
    "\n",
    "\n",
    "def fit(model, train_dl, val_dl, loss_fn, opt, epochs=3):\n",
    "    num_batch = len(train_dl)\n",
    "    for epoch in tnrange(epochs):      \n",
    "        y_true_train = list()\n",
    "        y_pred_train = list()\n",
    "        total_loss_train = 0          \n",
    "        \n",
    "        t = tqdm_notebook(iter(train_dl), leave=False, total=num_batch)\n",
    "        for cat, cont, y in t:\n",
    "            cat = cat.cuda()\n",
    "            cont = cont.cuda()\n",
    "            y = y.cuda()\n",
    "            \n",
    "            t.set_description(f'Epoch {epoch}')\n",
    "            \n",
    "            opt.zero_grad() #find where the grads are zero\n",
    "            pred = model(cat, cont)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            loss.backward() # do backprop\n",
    "            lr[epoch].append(opt.param_groups[0]['lr'])\n",
    "            tloss[epoch].append(loss.item())\n",
    "            opt.step()\n",
    "            #scheduler.step()\n",
    "            \n",
    "            \n",
    "            t.set_postfix(loss=loss.item())\n",
    "            \n",
    "            y_true_train += list(y.cpu().data.numpy())\n",
    "            y_pred_train += list(pred.cpu().data.numpy())\n",
    "            total_loss_train += loss.item()\n",
    "            \n",
    "        train_acc = rmse(y_true_train, y_pred_train)\n",
    "        train_loss = total_loss_train/len(train_dl) # len train_dl = 704. the calc is number of train examples (89991) / batch size (128)\n",
    "        \n",
    "        if val_dl:\n",
    "            y_true_val = list()\n",
    "            y_pred_val = list()\n",
    "            total_loss_val = 0\n",
    "            for cat, cont, y in tqdm_notebook(val_dl, leave=False):\n",
    "                cat = cat.cuda()\n",
    "                cont = cont.cuda()\n",
    "                y = y.cuda()\n",
    "                pred = model(cat, cont)\n",
    "                loss = loss_fn(pred, y)\n",
    "                \n",
    "                y_true_val += list(y.cpu().data.numpy())\n",
    "                y_pred_val += list(pred.cpu().data.numpy())\n",
    "                total_loss_val += loss.item()\n",
    "                vloss[epoch].append(loss.item())\n",
    "            valacc = rmse(y_true_val, y_pred_val)\n",
    "            valloss = total_loss_val/len(valdl)\n",
    "            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f} | val_loss: {valloss:.4f} val_rmse: {valacc:.4f}')\n",
    "        else:\n",
    "            print(f'Epoch {epoch}: train_loss: {train_loss:.4f} train_rmse: {train_acc:.4f}')\n",
    "    \n",
    "    return lr, tloss, vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, tloss, vloss = fit(model=model, train_dl=traindl, val_dl=valdl, loss_fn=nn.MSELoss(), opt=opt,  epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe0ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_tensor(df):\n",
    "    #device = get_device()\n",
    "    return torch.from_numpy(df.values).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build validation data for DataLoader\n",
    "val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "print(val_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(dataset=val_data, batch_size=16, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
